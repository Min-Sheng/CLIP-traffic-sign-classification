{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3bea00-1b20-4064-9d73-8e325a35fc5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da4ed7d-ce2c-4f22-b911-53f5e2f594a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c09f54-29bd-4f92-a16d-55f361720c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.44.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770301e-49db-47d1-8b29-48a8cddb4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e38a11-8ad5-4a46-a71d-9069e0563b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd4fff-a061-4756-b99c-7211a1ee3f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download ahemateja19bec1025/traffic-sign-dataset-classification\n",
    "!unzip traffic-sign-dataset-classification.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10653b0c-3f97-4f8b-a5cf-25834b8188c3",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14d185-5121-4f4f-a3af-445061123aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de467c6-8cad-49eb-939d-3cf598f49ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "from dataset_utils import val_transform\n",
    "from clip_model_utils import load_model_and_processor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca876f-7dd1-48b0-94c0-c9ad4c11639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21179b-d29a-42d1-8a58-e8ffe84fef22",
   "metadata": {},
   "source": [
    "## Load label name table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee0f75-f1fc-4a59-b0fb-46fc46081887",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_csv = './data/traffic_Data/corrected_labels.csv'\n",
    "label_map = pd.read_csv(label_csv)\n",
    "label_dict = dict(zip(label_map['ClassId'], label_map['Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0118f3-632c-4a5f-bfbc-5320a9803f9a",
   "metadata": {},
   "source": [
    "## Load test image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d1192-47a2-424b-9308-5b5e76aa3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data/traffic_Data'\n",
    "test_data_dir = Path(data_root) / 'TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9adc4-1bd5-45f6-95f1-69bd97f2c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = []\n",
    "\n",
    "for file_path in sorted(list(test_data_dir.iterdir())):\n",
    "    if file_path.suffix == '.png':\n",
    "        test_image_paths.append(str(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef12af1-3184-4a8d-bb45-fd9edde0b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(test_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff78f7a-5b75-4a91-8150-1eb2f7fc243b",
   "metadata": {},
   "source": [
    "## Load fine-tuned CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ff1b4-d64b-4ba2-b24b-e96b17fd1468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'ViT-B-16'\n",
    "model_path = '/home/vincentwu/clip_hw/HW1/results/ViT-B-16-openclip/best_model.pt'\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, processor, backend = load_model_and_processor(model_name, device=device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed8983-4fe4-4bc1-a938-4abca6f8acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompts\n",
    "class_ids_sorted = sorted(label_dict.keys())\n",
    "class_names = [label_dict[class_id] for class_id in class_ids_sorted]\n",
    "\n",
    "all_text_prompts = [f\"a photo of {name} traffic sign\" for name in class_names]\n",
    "\n",
    "# Encode prompts\n",
    "text_inputs = processor(all_text_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "if backend == 'huggingface':\n",
    "    text_features = model.get_text_features(**text_inputs)\n",
    "else:\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abb5ac-8c10-4291-82fa-d18929d3f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_image_paths)):\n",
    "    if i > 30:\n",
    "        break\n",
    "    image = Image.open(test_image_paths[i]).convert(\"RGB\")\n",
    "    image_tensor = val_transform(image) # use my own validation transform here\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "    if backend == 'huggingface':\n",
    "        image_features = model.get_image_features(pixel_values=image_tensor)\n",
    "    else:\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = (image_features @ text_features.T).squeeze()\n",
    "    pred_id = similarity.argmax().item()\n",
    "    pred_label = class_names[pred_id]\n",
    "\n",
    "    # Show results\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Prediction: {pred_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d4894-1199-40f5-be2c-1613be4f6f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
